{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image,\n",
    "    as illustrated in Figure 1.\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    # (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant')\n",
    "    # (≈ 1 line)\n",
    "    return X_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation\n",
    "    of the previous layer.\n",
    "\n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    # (≈ 1 line)\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    # Sum over all entries of the volume s.\n",
    "    # (≈ 1 line)\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    # (≈ 1 line)\n",
    "    Z = Z + float(b)\n",
    "    # (≈ 1 line)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "\n",
    "    for i in range(m):               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]   # Select ith training example's padded activation\n",
    "        for h in range(n_H):         # loop over vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\"\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):     # loop over horizontal axis of the output volume\n",
    "                # Find the horizontal start and end of the current \"slice\"\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C): # loop over channels (= #filters) of the output volume\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b,\n",
    "                    # to get back one output neuron\n",
    "                    weights = W[:, :, :, c]\n",
    "                    biases = b[:, :, :, c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :], W, b)\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2483972225.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class Solution {\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Solution {\n",
    "  public:\n",
    "    unordered_map<int,int>mx,mn;\n",
    "    int n;\n",
    "    vector<int> locns;\n",
    "\n",
    "    long long solve(vector<int> &types,int i,int prev_i)\n",
    "    {\n",
    "        if(i==n) return 0;\n",
    "        long long ans = LLONG_MAX;\n",
    "        if(i>0 && types[i]==types[i-1])\n",
    "        {\n",
    "            return solve(types,i+1,prev_i);\n",
    "        }\n",
    "        int prev;\n",
    "        if(prev_i==-1) prev = 0;\n",
    "        else\n",
    "        {\n",
    "            prev = locns[prev_i];\n",
    "        }\n",
    "        ans = min(ans,abs(locns[mx[types[i]]]-prev)+locns[mx[types[i]]]-locns[mn[types[i]]]+solve(types,i+1,mn[types[i]]));\n",
    "        ans = min(ans,abs(locns[mn[types[i]]]-prev)+locns[mx[types[i]]]-locns[mn[types[i]]]+solve(types,i+1,mx[types[i]]));\n",
    "        return ans;\n",
    "    }\n",
    "    long long minTime(int n, vector<int> &locations, vector<int> &types) {\n",
    "        // code here\n",
    "        n = locations.size();\n",
    "        locns = locations;\n",
    "        for(int i =0;i<n;i++)\n",
    "        {\n",
    "            if(mx.find(types[i])!=mx.end())\n",
    "            {\n",
    "                mx[types[i]] = locations[i];\n",
    "                mn[types[i]] = locations[i];\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                if(locations[i]>mx[types[i]]) mx[types[i]] = i;\n",
    "                if(locations[i]<mn[types[i]]) mn[types[i]] = i;\n",
    "            }\n",
    "        }\n",
    "        sort(types.begin(),types.end());\n",
    "        return solve(types,0,-1);\n",
    "    }\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
